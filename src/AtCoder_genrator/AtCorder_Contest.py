import requests
from bs4 import BeautifulSoup
import os
from dotenv import load_dotenv

Contest_List = [
    "abc403",
    "abc402",
    "abc401",
    "abc400"
]

CONTEST_ID = "abc398"
SUBMISSION_URL = f"https://atcoder.jp/contests/{CONTEST_ID}/submissions?f.Task=&f.LanguageName=C&f.Status=AC&f.User="

HEADERS = {
    "User-Agent": "Mozilla/5.0"
}

load_dotenv()
COOKIES = {
    "REVEL_SESSION": os.getenv("ATCODER_COOKIE")
}

right_cnt = 0

SAVE_DIR = f"data/raw_code/temp/AtCoder/{CONTEST_ID}"
os.makedirs(SAVE_DIR, exist_ok=True)

def fetch_submissions(max_pages=100):
    session = requests.Session()
    session.headers.update(HEADERS)
    session.cookies.update(COOKIES)

    all_submissions = []

    for page in range(1, max_pages + 1):
        url = f"{SUBMISSION_URL}&page={page}"
        res = session.get(url)

        print(f"ğŸ“„ í˜ì´ì§€ {page}: ìƒíƒœ ì½”ë“œ {res.status_code}")
        if res.status_code != 200:
            break
        if "Sign In" in res.text:
            print("âš ï¸ ë¡œê·¸ì¸ ìƒíƒœ ì•„ë‹˜. ì¿ í‚¤ í™•ì¸ í•„ìš”.")
            break

        soup = BeautifulSoup(res.text, 'html.parser')
        rows = soup.select("table tbody tr")
        if not rows:
            print("âœ… ë” ì´ìƒ ë°ì´í„° ì—†ìŒ. ì¢…ë£Œ.")
            break

        for row in rows:
            cols = row.find_all("td")
            if len(cols) < 3:
                continue

            user_cell = cols[2]
            user_link = user_cell.find("a", href=True)
            username = user_link['href'].split("/")[-1] if user_link else "unknown"

            detail_cell = cols[-1]
            detail_link = detail_cell.find("a", href=True)
            if detail_link and "/submissions/" in detail_link['href']:
                submission_id = detail_link['href'].split("/")[-1]
                all_submissions.append((username, submission_id))
                    

    return all_submissions

def download_submission_code(session, username, submission_id):
    global right_cnt
    detail_url = f"https://atcoder.jp/contests/{CONTEST_ID}/submissions/{submission_id}"
    res = session.get(detail_url)
    if res.status_code != 200:
        print(f"âŒ ì œì¶œ {submission_id} ì½”ë“œ ìš”ì²­ ì‹¤íŒ¨")
        return

    soup = BeautifulSoup(res.text, 'html.parser')
    code_block = soup.find("pre", id="submission-code")
    if not code_block:
        print(f"âš ï¸ ì½”ë“œ ë¸”ë¡ ì—†ìŒ: ì œì¶œ {submission_id}")
        return

    filename = f"{username}_{submission_id}.c"
    filepath = os.path.join(SAVE_DIR, filename)
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(code_block.text)
    print(f"âœ… ì €ì¥ë¨: {filename}")
    right_cnt += 1

if __name__ == "__main__":
    session = requests.Session()
    session.headers.update(HEADERS)
    session.cookies.update(COOKIES)

    submissions = fetch_submissions()

    for username, submission_id in submissions:
        download_submission_code(session, username, submission_id)
    
    print(f"ğŸ“¦ ì´ ì œì¶œ ìˆ˜: {len(submissions)}")
    print(f"âœ… ì„±ê³µì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œëœ ì œì¶œ ìˆ˜: {right_cnt}")