import requests
from bs4 import BeautifulSoup
import os
from dotenv import load_dotenv
import time
import random

USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)",
    "Mozilla/5.0 (X11; Linux x86_64)",
]

HEADERS = {
    "User-Agent": random.choice(USER_AGENTS)
}

load_dotenv()
COOKIES = {
    "REVEL_SESSION": os.getenv("ATCODER_COOKIE")
}

BASE_SAVE_DIR = "data/raw_code/temp/AtCoder"
right_cnt = 0

contest_403_list = []

def fetch_contests_from_archive(page=1, type=1, name="abc"):
    base_url = f"https://atcoder.jp/contests/archive?ratedType={type}&category=0&keyword="
    contests = []

    session = requests.Session()
    session.headers.update(HEADERS)
    session.cookies.update(COOKIES)
    
    url = f"{base_url}&page={page}"
    res = session.get(url)
    print(f"ğŸ“„ Contest Archive í˜ì´ì§€ {page}: ìƒíƒœ ì½”ë“œ {res.status_code}")

    soup = BeautifulSoup(res.text, 'html.parser')
    tag = f"table tbody tr td a[href^='/contests/{name}']"
    links = soup.select(tag)
    for link in links:
        contest_id = link['href'].split("/")[-1]
        contest_name = link.text.strip()
        contests.append((contest_id, contest_name))

    return contests

def fetch_submissions(session, contest_id, max_pages=100):
    global contest_403_list
    submission_url = f"https://atcoder.jp/contests/{contest_id}/submissions?f.Task=&f.LanguageName=C&f.Status=AC&f.User="
    all_submissions = []

    for page in range(1, max_pages + 1):
        url = f"{submission_url}&page={page}"
        res = session.get(url)

        print(f"ğŸ“„ {contest_id} - ì œì¶œ í˜ì´ì§€ {page}: ìƒíƒœ ì½”ë“œ {res.status_code}")
        if res.status_code != 200:
            contest_403_list.append(contest_id)
            break
        if "Sign In" in res.text:
            print("âš ï¸ ë¡œê·¸ì¸ ìƒíƒœ ì•„ë‹˜. ì¿ í‚¤ í™•ì¸ í•„ìš”.")
            break

        soup = BeautifulSoup(res.text, 'html.parser')
        rows = soup.select("table tbody tr")
        if not rows:
            print("â• ë” ì´ìƒ ë°ì´í„° ì—†ìŒ. ì¢…ë£Œ.")
            break

        for row in rows:
            cols = row.find_all("td")
            if len(cols) < 3:
                continue

            user_cell = cols[2]
            user_link = user_cell.find("a", href=True)
            username = user_link['href'].split("/")[-1] if user_link else "unknown"

            detail_cell = cols[-1]
            detail_link = detail_cell.find("a", href=True)
            if detail_link and "/submissions/" in detail_link['href']:
                submission_id = detail_link['href'].split("/")[-1]
                all_submissions.append((username, submission_id))

    return all_submissions

def download_submission_code(session, contest_id, username, submission_id):
    global right_cnt
    detail_url = f"https://atcoder.jp/contests/{contest_id}/submissions/{submission_id}"
    res = session.get(detail_url)
    if res.status_code != 200:
        print(f"âŒ ì œì¶œ {submission_id} ì½”ë“œ ìš”ì²­ ì‹¤íŒ¨")
        return

    soup = BeautifulSoup(res.text, 'html.parser')
    code_block = soup.find("pre", id="submission-code")
    if not code_block:
        print(f"âš ï¸ ì½”ë“œ ë¸”ë¡ ì—†ìŒ: ì œì¶œ {submission_id}")
        
        return

    save_dir = os.path.join(BASE_SAVE_DIR, contest_id)
    os.makedirs(save_dir, exist_ok=True)

    filename = f"{username}_{submission_id}.c"
    filepath = os.path.join(save_dir, filename)
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(code_block.text)
    print(f"âœ… ì €ì¥ë¨: {contest_id}/{filename}")
    right_cnt += 1
    time.sleep(1)

if __name__ == "__main__":
    # ì´ ê³³ ìˆ˜ì • ABC, ARC, AGC, AHC
    # page, type, name
    contests = fetch_contests_from_archive(3, 2, "arc") 
    session = requests.Session()
    session.headers.update(HEADERS)
    session.cookies.update(COOKIES)

    for contest_id, contest_name in contests:
        save_dir = os.path.join(BASE_SAVE_DIR, contest_id)
        if os.path.exists(save_dir):
            print(f"â© ì´ë¯¸ ìˆ˜ì§‘ëœ ëŒ€íšŒì…ë‹ˆë‹¤. ê±´ë„ˆëœ€: {contest_id}")
            continue

        print(f"\nğŸ” {contest_name} ({contest_id}) ìˆ˜ì§‘ ì‹œì‘")
        submissions = fetch_submissions(session, contest_id)

        if not submissions:
            print(f"ğŸ“­ ìˆ˜ì§‘ ê°€ëŠ¥í•œ ì œì¶œ ì—†ìŒ ë˜ëŠ” ì˜¤ë¥˜ ë°œìƒ: {contest_id}")
            continue

        for username, submission_id in submissions:
            download_submission_code(session, contest_id, username, submission_id)

    print(f"\nğŸ“¦ ì´ ì €ì¥ëœ ì½”ë“œ ìˆ˜: {right_cnt}")
    
    print("ğŸš« 403 ì—ëŸ¬ ë°œìƒí•œ ëŒ€íšŒ ëª©ë¡:")
    for contest_id in contest_403_list:
        print(f"- {contest_id}")

