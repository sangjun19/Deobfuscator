import requests
import base64
import os
import random
import time
from dotenv import load_dotenv
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

load_dotenv()
api_url = "https://api.github.com/search/code"
github_token = os.getenv('GITHUB_TOKEN')
LANGUAGE = "c++"

# GitHub API ì†ë„ ì œí•œ ì¶”ì ì„ ìœ„í•œ ê¸€ë¡œë²Œ ë³€ìˆ˜
rate_limit_remaining = 60  # ì´ˆê¸°ê°’: ê°€ì •(ìµœëŒ€ ë¶„ë‹¹ 60ê°œ ìš”ì²­)
rate_limit_reset = 0       # ë‹¤ìŒ ì¬ì„¤ì • ì‹œê°„(Unix íƒ€ì„ìŠ¤íƒ¬í”„)

# íŒŒì¼ í¬ê¸°ë‚˜ ë‹¤ë¥¸ ê¸°ì¤€ìœ¼ë¡œ í•˜ìœ„ ì¿¼ë¦¬ ë¶„í• 
SUB_QUERIES = [
    "+extension:c++",
    "+filename:main",
    "+filename:util",
    "+filename:core",
    "+case",
    "+default",
    "+break",
    "+path:src/",
    "+path:include/",
    "+path:lib/",
    "+created:2015-01-01..2015-12-31",
    "+created:2016-01-01..2016-12-31",
    "+created:2017-01-01..2017-12-31",
    "+created:2018-01-01..2018-12-31",
    "+created:2019-01-01..2019-12-31",
    "+created:2020-01-01..2020-12-31",
    "+created:2021-01-01..2021-12-31",
    "+created:2022-01-01..2022-12-31",
    "+created:2023-01-01..2023-12-31",
    "+created:2024-01-01..2024-12-31",
    "+created:>2025-01-01",
    "+size:<1000"
    "+size:>=1000..<2000",
    "+size:>=2000..<3000",
    "+size:>=3000..<4000",
    "+size:>=4000..<5000",
    "+stars:>100",
    "+stars:<=100",
]

# ê¸°ë³¸ ì¿¼ë¦¬
base_query = f"switch language:{LANGUAGE}"

headers = {
    "Authorization": f"token {github_token}",
    "Accept": "application/vnd.github.v3+json"
}

params = {
    "q": base_query,
    "per_page": 100
}

def requests_retry_session(retries=3, backoff_factor=0.3, status_forcelist=(500, 502, 504)):
    session = requests.Session()
    retry = Retry(
        total=retries,
        read=retries,
        connect=retries,
        backoff_factor=backoff_factor,
        status_forcelist=status_forcelist,
    )
    adapter = HTTPAdapter(max_retries=retry)
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    return session

def check_rate_limit(response):
    """GitHub API ì‘ë‹µ í—¤ë”ì—ì„œ ì†ë„ ì œí•œ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤"""
    global rate_limit_remaining, rate_limit_reset
    
    # X-RateLimit-Remaining: ë‚¨ì€ ìš”ì²­ ìˆ˜
    if 'X-RateLimit-Remaining' in response.headers:
        rate_limit_remaining = int(response.headers['X-RateLimit-Remaining'])
    
    # X-RateLimit-Reset: ì†ë„ ì œí•œ ì¬ì„¤ì • ì‹œê°„(Unix íƒ€ì„ìŠ¤íƒ¬í”„)
    if 'X-RateLimit-Reset' in response.headers:
        rate_limit_reset = int(response.headers['X-RateLimit-Reset'])
    
    print(f"ë‚¨ì€ API ìš”ì²­ ìˆ˜: {rate_limit_remaining}, ì¬ì„¤ì • ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(rate_limit_reset))}")
    
    # ë‚¨ì€ ìš”ì²­ ìˆ˜ê°€ 10 ì´í•˜ë©´ ê²½ê³ 
    if rate_limit_remaining <= 10:
        print(f"âš ï¸ ê²½ê³ : API ìš”ì²­ ì œí•œì— ê·¼ì ‘í–ˆìŠµë‹ˆë‹¤! ë‚¨ì€ ìš”ì²­ ìˆ˜: {rate_limit_remaining}")

def wait_if_rate_limited():
    """ì†ë„ ì œí•œì— ê·¼ì ‘í•˜ë©´ ëŒ€ê¸°í•©ë‹ˆë‹¤"""
    global rate_limit_remaining, rate_limit_reset
    
    # ë‚¨ì€ ìš”ì²­ ìˆ˜ê°€ 5 ì´í•˜ë©´ ì¬ì„¤ì • ì‹œê°„ê¹Œì§€ ëŒ€ê¸°
    if rate_limit_remaining <= 5:
        current_time = time.time()
        wait_time = max(0, rate_limit_reset - current_time) + 5  # 5ì´ˆ ì¶”ê°€ ì—¬ìœ 
        
        if wait_time > 0:
            print(f"ğŸ•’ ì†ë„ ì œí•œì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. {wait_time:.1f}ì´ˆ ëŒ€ê¸° ì¤‘...")
            time.sleep(wait_time)
            # ëŒ€ê¸° í›„ ì†ë„ ì œí•œ ì¬ì„¤ì •
            rate_limit_remaining = 60
            print("ì†ë„ ì œí•œì´ ì¬ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤. ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")

def exponential_backoff(attempt, base_delay=2, max_delay=120, jitter=True):
    """ì§€ìˆ˜ ë°±ì˜¤í”„ ëŒ€ê¸° ì‹œê°„ì„ ê³„ì‚°í•©ë‹ˆë‹¤"""
    delay = min(base_delay * (2 ** attempt), max_delay)
    
    # ì§€í„° ì¶”ê°€ (ë¬´ì‘ìœ„ì„±ìœ¼ë¡œ ì„œë²„ ë¶€í•˜ ë¶„ì‚°)
    if jitter:
        delay *= random.uniform(0.8, 1.2)
    
    return delay

def search_github_code():
    results = []
    total_count = 0
    session = requests_retry_session()
    
    # ê° í•˜ìœ„ ì¿¼ë¦¬ì— ëŒ€í•´ ê²€ìƒ‰ ìˆ˜í–‰
    for subquery_idx, filter_param in enumerate(SUB_QUERIES):
        print(f"ì²˜ë¦¬ ì¤‘ì¸ í•˜ìœ„ ì¿¼ë¦¬ {subquery_idx + 1}/{len(SUB_QUERIES)}: {filter_param}")
        
        # ì†ë„ ì œí•œ í™•ì¸ ë° í•„ìš” ì‹œ ëŒ€ê¸°
        wait_if_rate_limited()
        
        # í˜„ì¬ í•˜ìœ„ ì¿¼ë¦¬ì— í•„í„° ì¶”ê°€
        params["q"] = base_query + " " + filter_param.replace("+", " ")
        
        page = 1
        subquery_count = 0
        
        # í˜„ì¬ í•˜ìœ„ ì¿¼ë¦¬ì˜ ì´ í˜ì´ì§€ ìˆ˜ ê³„ì‚° (ìµœëŒ€ 3ë²ˆ ì¬ì‹œë„)
        for attempt in range(3):
            try:
                initial_response = session.get(api_url, headers=headers, params=params)
                initial_response.raise_for_status()
                check_rate_limit(initial_response)  # ì†ë„ ì œí•œ í™•ì¸
                
                initial_data = initial_response.json()
                total_results = initial_data.get("total_count", 0)
                max_pages = min(10, (total_results + 99) // 100)  # ìµœëŒ€ 10í˜ì´ì§€(1000ê°œ)ê¹Œì§€ë§Œ ì²˜ë¦¬
                print(f"í˜„ì¬ í•˜ìœ„ ì¿¼ë¦¬ì˜ ì´ ê²°ê³¼: {total_results}, ì²˜ë¦¬í•  í˜ì´ì§€ ìˆ˜: {max_pages}")
                
                # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ë‹¤ìŒ ì¿¼ë¦¬ë¡œ ë„˜ì–´ê°
                if total_results == 0:
                    break
                
                # ì²« í˜ì´ì§€ì— ìˆëŠ” í•­ëª© ì²˜ë¦¬
                if "items" in initial_data and initial_data["items"]:
                    process_items(session, initial_data["items"], results, subquery_count, total_count)
                    subquery_count = len(results) - total_count
                    total_count = len(results)
                
                # í˜ì´ì§€ ì²˜ë¦¬ ë°˜ë³µë¬¸ ì‹œì‘
                page = 2  # ì²« í˜ì´ì§€ëŠ” ì´ë¯¸ ì²˜ë¦¬í–ˆìœ¼ë¯€ë¡œ 2ë¶€í„° ì‹œì‘
                
                break  # ì„±ê³µí•˜ë©´ ì¬ì‹œë„ ì¤‘ë‹¨
                
            except requests.exceptions.RequestException as e:
                wait_time = exponential_backoff(attempt)
                
                if attempt < 2:  # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ ê²½ìš°
                    print(f"ì´ˆê¸° ì¿¼ë¦¬ ì˜¤ë¥˜: {e} - {wait_time:.1f}ì´ˆ í›„ ì¬ì‹œë„ ({attempt+1}/3)...")
                    time.sleep(wait_time)
                else:
                    print(f"ì´ˆê¸° ì¿¼ë¦¬ ì˜¤ë¥˜: {e} - ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. ë‹¤ìŒ ì¿¼ë¦¬ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")
                    break
        
        # 2í˜ì´ì§€ë¶€í„° max_pagesê¹Œì§€ ì²˜ë¦¬
        while page <= max_pages:
            # ì†ë„ ì œí•œ í™•ì¸ ë° í•„ìš” ì‹œ ëŒ€ê¸°
            wait_if_rate_limited()
            
            params["page"] = page
            retry_count = 0
            max_retries = 5
            
            while retry_count < max_retries:
                try:
                    print(f"í˜ì´ì§€ {page}/{max_pages} ì²˜ë¦¬ ì¤‘...")
                    response = session.get(api_url, headers=headers, params=params)
                    response.raise_for_status()
                    check_rate_limit(response)  # ì†ë„ ì œí•œ í™•ì¸
                    
                    data = response.json()
                    
                    if "items" not in data or not data["items"]:
                        print("ë” ì´ìƒ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
                        break
                    
                    # í•­ëª© ì²˜ë¦¬
                    new_items = process_items(session, data["items"], results, subquery_count, total_count)
                    subquery_count += new_items
                    total_count += new_items
                    
                    page += 1
                    retry_count = 0  # ì„±ê³µí•˜ë©´ ì¬ì‹œë„ ì¹´ìš´í„° ë¦¬ì…‹
                    
                    # ê° í˜ì´ì§€ ìš”ì²­ ì‚¬ì´ì— ëŒ€ê¸° (ì†ë„ ì œí•œ ì˜ˆë°©)
                    time.sleep(3)  # 2ì´ˆì—ì„œ 3ì´ˆë¡œ ì¦ê°€
                    break  # ì„±ê³µí–ˆìœ¼ë¯€ë¡œ ì¬ì‹œë„ ë£¨í”„ ì¢…ë£Œ
                    
                except requests.exceptions.RequestException as e:
                    retry_count += 1
                    wait_time = exponential_backoff(retry_count, base_delay=5)
                    
                    if "rate limit" in str(e).lower() or response.status_code == 403:
                        print(f"API ì†ë„ ì œí•œì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. {wait_time:.1f}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„ ({retry_count}/{max_retries})...")
                        time.sleep(wait_time)
                    elif retry_count < max_retries:
                        print(f"í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e} - {wait_time:.1f}ì´ˆ í›„ ì¬ì‹œë„ ({retry_count}/{max_retries})...")
                        time.sleep(wait_time)
                    else:
                        print(f"í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {e}")
                        break
        
        print(f"í•˜ìœ„ ì¿¼ë¦¬ {subquery_idx + 1} ì™„ë£Œ: {subquery_count}ê°œ íŒŒì¼ ì²˜ë¦¬ë¨")
        
        # API ì†ë„ ì œí•œ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸°
        if subquery_idx < len(SUB_QUERIES) - 1:
            wait_time = 15  # 10ì´ˆì—ì„œ 15ì´ˆë¡œ ì¦ê°€
            print(f"ë‹¤ìŒ í•˜ìœ„ ì¿¼ë¦¬ ì „ {wait_time}ì´ˆ ëŒ€ê¸° ì¤‘...")
            time.sleep(wait_time)
    
    return results

def process_items(session, items, results, subquery_count, total_count):
    """í•­ëª© ëª©ë¡ì„ ì²˜ë¦¬í•˜ê³  ìƒˆë¡œ ì¶”ê°€ëœ í•­ëª© ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤"""
    added_count = 0
    
    for item in items:
        file_url = item["url"]
        
        # íŒŒì¼ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (ìµœëŒ€ 3ë²ˆ ì¬ì‹œë„)
        for attempt in range(3):
            try:
                # ì†ë„ ì œí•œ í™•ì¸ ë° í•„ìš” ì‹œ ëŒ€ê¸°
                wait_if_rate_limited()
                
                file_content = get_file_content(session, file_url)
                if file_content and "switch" in file_content:
                    result = {
                        "repo": item["repository"]["full_name"],
                        "file": item["path"],
                        "code": file_content
                    }
                    results.append(result)
                    save_to_file(result)
                    added_count += 1
                    print(f"{total_count + added_count} : Repository: {result['repo']}")
                
                # ê° íŒŒì¼ ìš”ì²­ ì‚¬ì´ì— ì§§ì€ ëŒ€ê¸°
                time.sleep(1)
                break  # ì„±ê³µí–ˆìœ¼ë¯€ë¡œ ì¬ì‹œë„ ë£¨í”„ ì¢…ë£Œ
                
            except Exception as e:
                wait_time = exponential_backoff(attempt)
                
                if "403" in str(e) and attempt < 2:  # 403 ì—ëŸ¬ì¸ ê²½ìš° ë” ê¸¸ê²Œ ëŒ€ê¸°
                    print(f"íŒŒì¼ ë‚´ìš© ì²˜ë¦¬ ì¤‘ ê¶Œí•œ ì˜¤ë¥˜: {e} - {wait_time:.1f}ì´ˆ í›„ ì¬ì‹œë„ ({attempt+1}/3)...")
                    time.sleep(wait_time * 2)  # 403 ì—ëŸ¬ëŠ” ë‘ ë°° ê¸¸ê²Œ ëŒ€ê¸°
                elif attempt < 2:  # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ ê²½ìš°
                    print(f"íŒŒì¼ ë‚´ìš© ì²˜ë¦¬ ì˜¤ë¥˜: {e} - {wait_time:.1f}ì´ˆ í›„ ì¬ì‹œë„ ({attempt+1}/3)...")
                    time.sleep(wait_time)
                else:
                    print(f"íŒŒì¼ ë‚´ìš© ì²˜ë¦¬ ì˜¤ë¥˜: {e} - ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. ë‹¤ìŒ í•­ëª©ìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")
    
    return added_count

def get_file_content(session, file_url):
    """íŒŒì¼ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ê³  ì†ë„ ì œí•œ ì •ë³´ë¥¼ í™•ì¸í•©ë‹ˆë‹¤"""
    response = session.get(file_url, headers=headers)
    response.raise_for_status()
    
    # ì†ë„ ì œí•œ í™•ì¸
    check_rate_limit(response)
    
    data = response.json()
    if "content" in data:
        return base64.b64decode(data["content"]).decode("utf-8", errors="replace")
    return None

def save_to_file(result):
    directory = f"./data/github_api/github_switch_codes_{LANGUAGE}"
    if not os.path.exists(directory):
        os.makedirs(directory)

    repo_name = result["repo"].replace("/", "_")
    file_name = os.path.basename(result["file"])
    save_name = f"{repo_name}_{file_name}"

    with open(os.path.join(directory, save_name), "w", encoding="utf-8", errors="replace") as f:
        f.write(result['code'])

if __name__ == "__main__":
    results = search_github_code()
    print(f"ì´ {len(results)}ê°œ íŒŒì¼ì´ 'github_switch_codes_{LANGUAGE}' ë””ë ‰í† ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
